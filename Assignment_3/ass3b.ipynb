{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3B: Implementing recurrent neural networks in PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "\n",
    "# Other Libraty Imports\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Local Imports\n",
    "from data_rnn import load_imdb\n",
    "\n",
    "# Constants\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Classification: data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Classification, baseline model\n",
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Writing your own Elman RNN\n",
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Using Torch's RNNs\n",
    "### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Autoregressive models\n",
    "### Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from data_rnn import load_ndfa\n",
    "from data_rnn import load_brackets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# data\n",
    "x_train_ndfa, (i2w_ndfa, w2i_ndfa) = load_ndfa(n=150_000)\n",
    "x_train_brack, (i2w_brack, w2i_brack) = load_brackets(n=150_000)\n",
    "\n",
    "# code"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ss\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create batch separation lines, Append start and end\n",
    "\n",
    "print(''.join([i2w_ndfa[i] for i in  x_train_ndfa[0]]) )\n",
    "\n",
    "x_train, (i2w, w2i) = load_brackets(n=150_000)\n",
    "\n",
    "batches = []\n",
    "batch_positions = [0]\n",
    "# max_tokens = 50000\n",
    "max_batch_length = 64\n",
    "# token_multiplier = 0\n",
    "token_counter = 0\n",
    "y_train = []\n",
    "for i in range(len(x_train)):\n",
    "    x_train[i].insert(0,w2i[\".start\"])\n",
    "    x_train[i].append(w2i[\".end\"])\n",
    "    y_train.append(x_train[i].copy())\n",
    "    y_train[i][1] = w2i[\".unk\"]\n",
    "\n",
    "    if token_counter >= max_batch_length:\n",
    "        token_counter = 0\n",
    "        batch_positions.append(i)\n",
    "        # token_multiplier += 1\n",
    "    batches.append(len(batch_positions))\n",
    "    # token_counter += len(x_train[i])\n",
    "    token_counter += 1\n",
    "token_sizes = {}\n",
    "for i in x_train:\n",
    "    if token_sizes.get(len(i)):\n",
    "        token_sizes[len(i)] += 1\n",
    "    else:\n",
    "        token_sizes[len(i)] = 1\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "#Separate batches into tensors\n",
    "x_batches = []\n",
    "y_batches = []\n",
    "for i in range(len(batch_positions)-1):\n",
    "    batch = x_train[batch_positions[i]:batch_positions[i+1]].copy()\n",
    "    y_batch = y_train[batch_positions[i]:batch_positions[i+1]].copy()\n",
    "    tok_len = len(batch[-1])\n",
    "    for j in range(len(x_train[batch_positions[i]:batch_positions[i+1]])):\n",
    "        if len(batch[j]) < tok_len:\n",
    "            _temp_list = [w2i[\".pad\"]]*(tok_len-len(batch[j]))\n",
    "            batch[j].extend(_temp_list)\n",
    "            y_batch[j].extend(_temp_list)\n",
    "    x_batches.append(torch.tensor(batch))\n",
    "    y_batches.append(torch.tensor(batch))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class RecurrentNN(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dim = 32, h_size = 16, lay_num = 1):\n",
    "        super(RecurrentNN, self).__init__()\n",
    "        self.embed_num = len(w2i)\n",
    "        self.emb_dim = emb_dim\n",
    "        self.h_size = h_size\n",
    "        self.lay_num = lay_num\n",
    "        self.embed = nn.Embedding(num_embeddings=self.embed_num, embedding_dim=self.emb_dim)\n",
    "        self.lstm = nn.LSTM(input_size=self.emb_dim, hidden_size=self.h_size, num_layers=self.lay_num, batch_first=True)\n",
    "        self.lin = nn.Linear(self.h_size, self.embed_num)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        x, states = self.lstm(x)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RecurrentNN(\n",
      "  (embed): Embedding(6, 32)\n",
      "  (lstm): LSTM(32, 16, batch_first=True)\n",
      "  (lin): Linear(in_features=16, out_features=6, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create RNN object\n",
    "\n",
    "net = RecurrentNN(lay_num=1)\n",
    "print(net)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 6,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_6892/3152997427.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[0mcriterion\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mCrossEntropyLoss\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m \u001B[0moptimizer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0moptim\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mSGD\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnet\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mparameters\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlr\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0.001\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmomentum\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0.9\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'net' is not defined"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 1.266\n",
      "[2,  2000] loss: 0.833\n",
      "[3,  2000] loss: 0.693\n",
      "[4,  2000] loss: 0.593\n",
      "[5,  2000] loss: 0.528\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    random.shuffle(x_batches)\n",
    "    for i, data in enumerate(x_batches, 0):\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        zeros = torch.zeros(64,1)\n",
    "        target = data.clone()\n",
    "        target = target.T[1:]\n",
    "        target = target.T\n",
    "        target = torch.cat((target, zeros.long()), 1)\n",
    "        outputs = net(data)\n",
    "        outputs = outputs.reshape(outputs.shape[0],outputs.shape[2],outputs.shape[1])\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 5, 5, 4, 4, 2]]\n"
     ]
    }
   ],
   "source": [
    "max_sequence_length = 160\n",
    "seq = [[w2i['.start'], w2i['('], w2i['('], w2i[')']]]\n",
    "seq_len = len(seq[0])\n",
    "while seq[0][-1] != w2i['.end'] and seq_len <= max_sequence_length:\n",
    "    seq_tens = torch.tensor(seq)\n",
    "    seq_tens.shape\n",
    "    outputs = net(seq_tens)\n",
    "    _output = []\n",
    "    dst = outputs[0,-1,: ].detach().numpy()\n",
    "    prob_dist = np.exp(dst)/sum(np.exp(dst))\n",
    "    seq_len += 1\n",
    "    vals_w2i = list(w2i.values())\n",
    "    # seq[0].append(int(np.random.choice(a=vals_w2i, p=prob_dist)))\n",
    "    seq[0].append(np.random.choice(vals_w2i,p=prob_dist))\n",
    "print(seq)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".start(()).end\n"
     ]
    }
   ],
   "source": [
    "print(''.join([i2w[i] for i in  seq[0]]) )\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "PATH = './dyck_model_ep5_seed42_overpredicting_shuffle_per_epoch.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "# Sampling definition\n",
    "import torch.distributions as dist\n",
    "def sample(lnprobs, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Sample an element from a categorical distribution\n",
    "    :param lnprobs: Outcome logits\n",
    "    :param temperature: Sampling temperature. 1.0 follows the given\n",
    "    distribution, 0.0 returns the maximum probability element.\n",
    "    :return: The index of the sampled element.\n",
    "    \"\"\"\n",
    "    if temperature == 0.0:\n",
    "        return lnprobs.argmax()\n",
    "    p = F.softmax(lnprobs / temperature, dim=0)\n",
    "    cd = dist.Categorical(p)\n",
    "    return cd.sample()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RecurrentNN(\n",
      "  (embed): Embedding(6, 32)\n",
      "  (lstm): LSTM(32, 16, batch_first=True)\n",
      "  (lin): Linear(in_features=16, out_features=6, bias=True)\n",
      ")\n",
      "[1,  2000] loss: 1.264\n",
      "Finished Epoch 1\n",
      ".start(().end\n",
      ".start(().unk).end\n",
      ".start(())(.start.start.unk()()().end\n",
      ".start(())().unk.unk).end\n",
      ".start(()()().pad(.pad.pad.start.unk.unk.pad)(.end\n",
      ".start(())()().pad)())).pad(.end\n",
      ".start(()).unk.end\n",
      ".start(())().unk.end\n",
      ".start(())(.end\n",
      ".start(()()().pad)).end\n",
      "[2,  2000] loss: 0.784\n",
      "Finished Epoch 2\n",
      ".start(()()).end\n",
      ".start(()()).pad.start.unk.pad)))(.end\n",
      ".start(()()()).end\n",
      ".start(()).end\n",
      ".start(()).unk))).unk.end\n",
      ".start(().unk.pad.unk).start.unk.end\n",
      ".start(()().pad.pad).end\n",
      ".start(()).start.unk.end\n",
      ".start(().end\n",
      ".start(().end\n",
      "[3,  2000] loss: 0.612\n",
      "Finished Epoch 3\n",
      ".start(()).end\n",
      ".start(().start()()().pad.end\n",
      ".start(()).end\n",
      ".start(()))).pad.end\n",
      ".start(()().unk).end\n",
      ".start(()()).pad.pad.pad)).end\n",
      ".start(()).end\n",
      ".start(()).unk.end\n",
      ".start(().end\n",
      ".start(()()).end\n",
      "[4,  2000] loss: 0.544\n",
      "Finished Epoch 4\n",
      ".start(())).end\n",
      ".start(()())).end\n",
      ".start(())).end\n",
      ".start(()).pad().pad)).pad.pad)).start.unk.pad.pad.unk.pad.start.unk.pad.unk.pad.start.unk.unk).pad.unk.end\n",
      ".start(().unk))).pad.unk))).end\n",
      ".start(().start.unk()).end\n",
      ".start(().start.unk)(.pad.start.unk.pad).end\n",
      ".start(()()).start.unk.end\n",
      ".start(())))().end\n",
      ".start(()).end\n",
      "[5,  2000] loss: 0.498\n",
      "Finished Epoch 5\n",
      ".start(()()())(.end\n",
      ".start(()).end\n",
      ".start(()).end\n",
      ".start(())).end\n",
      ".start(())).end\n",
      ".start(()).end\n",
      ".start(().end\n",
      ".start(())).pad.end\n",
      ".start(()).end\n",
      ".start(()).unk.pad.pad.pad.pad.pad.pad).pad).pad.unk.end\n",
      "[6,  2000] loss: 0.474\n",
      "Finished Epoch 6\n",
      ".start(()).end\n",
      ".start(()).end\n",
      ".start(()).end\n",
      ".start(()())))).pad).end\n",
      ".start(()())).pad.end\n",
      ".start(().end\n",
      ".start(()).end\n",
      ".start(()).end\n",
      ".start(()).end\n",
      ".start(()).end\n",
      "[7,  2000] loss: 0.445\n",
      "Finished Epoch 7\n",
      ".start(().end\n",
      ".start(().start.unk.pad)).end\n",
      ".start(()).end\n",
      ".start(()).unk.pad.pad.pad)).start.pad.end\n",
      ".start(()).end\n",
      ".start(()).end\n",
      ".start(()).end\n",
      ".start(()).end\n",
      ".start(().end\n",
      ".start(()()))).pad(.end\n",
      "[8,  2000] loss: 0.431\n",
      "Finished Epoch 8\n",
      ".start(())).end\n",
      ".start(()).end\n",
      ".start(()).end\n",
      ".start(()).end\n",
      ".start(()).end\n",
      ".start(()).pad.pad)).end\n",
      ".start(()).end\n",
      ".start(().end\n",
      ".start(()).end\n",
      ".start(()).end\n",
      "[9,  2000] loss: 0.422\n",
      "Finished Epoch 9\n",
      ".start(()).end\n",
      ".start(()).unk.pad.pad.pad).pad)).end\n",
      ".start(()).end\n",
      ".start(()).end\n",
      ".start(()).end\n",
      ".start(().end\n",
      ".start(()).end\n",
      ".start(()).end\n",
      ".start(()()).end\n",
      ".start(()).end\n",
      "[10,  2000] loss: 0.410\n",
      "Finished Epoch 10\n",
      ".start(()())).end\n",
      ".start(()).end\n",
      ".start(()).end\n",
      ".start(()).end\n",
      ".start(()).end\n",
      ".start(()).end\n",
      ".start(()).unk).end\n",
      ".start(().end\n",
      ".start(()).end\n",
      ".start(()).end\n"
     ]
    }
   ],
   "source": [
    "# Code\n",
    "\n",
    "net = RecurrentNN(lay_num=1)\n",
    "print(net)\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "max_sequence_length = 160\n",
    "\n",
    "import random\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    random.seed(42)\n",
    "    random.shuffle(x_batches)\n",
    "    for i, data in enumerate(x_batches, 0):\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        zeros = torch.zeros(64,1)\n",
    "        target = data.clone()\n",
    "        target = target.T[1:]\n",
    "        target = target.T\n",
    "        target = torch.cat((target, zeros.long()), 1)\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(data)\n",
    "        outputs = outputs.reshape(outputs.shape[0],outputs.shape[2],outputs.shape[1])\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "    print(f'Finished Epoch {epoch+1}')\n",
    "    for seeds in range(10):\n",
    "        seq = [[w2i['.start'], w2i['('], w2i['('], w2i[')']]]\n",
    "        seq_len = len(seq[0])\n",
    "        while seq[0][-1] != w2i['.end'] and seq_len <= max_sequence_length:\n",
    "            seq_tens = torch.tensor(seq)\n",
    "            outputs = net(seq_tens)\n",
    "            dst = outputs[0,-1,: ]\n",
    "            prob_dist = sample(dst)\n",
    "            seq_len += 1\n",
    "            vals_w2i = list(w2i.values())\n",
    "            # seq[0].append(int(np.random.choice(a=vals_w2i, p=prob_dist)))\n",
    "            seq[0].append(vals_w2i[prob_dist])\n",
    "        print(''.join([i2w[i] for i in  seq[0]]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "PATH = './q6_dyck_model_ep10_seed42_70_percent.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RecurrentNN(\n",
      "  (embed): Embedding(6, 32)\n",
      "  (lstm): LSTM(32, 16, batch_first=True)\n",
      "  (lin): Linear(in_features=16, out_features=6, bias=True)\n",
      ")\n",
      "[1,  2000] loss: 194.769\n",
      "Finished Epoch 1\n",
      "[2,  2000] loss: 177.682\n",
      "Finished Epoch 2\n",
      "[3,  2000] loss: 205.706\n",
      "Finished Epoch 3\n",
      "[4,  2000] loss: 223.880\n",
      "Finished Epoch 4\n",
      "[5,  2000] loss: 209.553\n",
      "Finished Epoch 5\n",
      "[6,  2000] loss: 182.446\n",
      "Finished Epoch 6\n",
      "[7,  2000] loss: 168.475\n",
      "Finished Epoch 7\n",
      "[8,  2000] loss: 158.291\n",
      "Finished Epoch 8\n",
      "[9,  2000] loss: 157.141\n",
      "Finished Epoch 9\n",
      "[10,  2000] loss: 163.899\n",
      "Finished Epoch 10\n",
      "[11,  2000] loss: 153.922\n",
      "Finished Epoch 11\n",
      "[12,  2000] loss: 150.888\n",
      "Finished Epoch 12\n",
      "[13,  2000] loss: 147.778\n",
      "Finished Epoch 13\n",
      "[14,  2000] loss: 147.729\n",
      "Finished Epoch 14\n",
      "[15,  2000] loss: 141.561\n",
      "Finished Epoch 15\n",
      "[16,  2000] loss: 145.783\n",
      "Finished Epoch 16\n",
      "[17,  2000] loss: 137.015\n",
      "Finished Epoch 17\n",
      "[18,  2000] loss: 143.662\n",
      "Finished Epoch 18\n",
      "[19,  2000] loss: 135.463\n",
      "Finished Epoch 19\n",
      "[20,  2000] loss: 134.946\n",
      "Finished Epoch 20\n",
      "[21,  2000] loss: 136.729\n",
      "Finished Epoch 21\n",
      "[22,  2000] loss: 133.464\n",
      "Finished Epoch 22\n",
      "[23,  2000] loss: 132.583\n",
      "Finished Epoch 23\n",
      "[24,  2000] loss: 126.033\n",
      "Finished Epoch 24\n",
      "[25,  2000] loss: 126.573\n",
      "Finished Epoch 25\n",
      "[26,  2000] loss: 133.794\n",
      "Finished Epoch 26\n",
      "[27,  2000] loss: 131.393\n",
      "Finished Epoch 27\n",
      "[28,  2000] loss: 128.416\n",
      "Finished Epoch 28\n",
      "[29,  2000] loss: 127.207\n",
      "Finished Epoch 29\n",
      "[30,  2000] loss: 130.011\n",
      "Finished Epoch 30\n",
      "[31,  2000] loss: 131.208\n",
      "Finished Epoch 31\n",
      "[32,  2000] loss: 130.408\n",
      "Finished Epoch 32\n",
      "[33,  2000] loss: 131.254\n",
      "Finished Epoch 33\n",
      "[34,  2000] loss: 129.722\n",
      "Finished Epoch 34\n",
      "[35,  2000] loss: 123.560\n",
      "Finished Epoch 35\n",
      "[36,  2000] loss: 124.625\n",
      "Finished Epoch 36\n",
      "[37,  2000] loss: 125.958\n",
      "Finished Epoch 37\n",
      "[38,  2000] loss: 131.632\n",
      "Finished Epoch 38\n",
      "[39,  2000] loss: 134.846\n",
      "Finished Epoch 39\n",
      "[40,  2000] loss: 132.124\n",
      "Finished Epoch 40\n",
      "[41,  2000] loss: 128.112\n",
      "Finished Epoch 41\n",
      "[42,  2000] loss: 132.451\n",
      "Finished Epoch 42\n",
      "[43,  2000] loss: 125.722\n",
      "Finished Epoch 43\n",
      "[44,  2000] loss: 137.186\n",
      "Finished Epoch 44\n",
      "[45,  2000] loss: 132.776\n",
      "Finished Epoch 45\n",
      "[46,  2000] loss: 137.685\n",
      "Finished Epoch 46\n",
      "[47,  2000] loss: 130.627\n",
      "Finished Epoch 47\n",
      "[48,  2000] loss: 131.148\n",
      "Finished Epoch 48\n",
      "[49,  2000] loss: 138.838\n",
      "Finished Epoch 49\n",
      "[50,  2000] loss: 130.924\n",
      "Finished Epoch 50\n"
     ]
    }
   ],
   "source": [
    "# code\n",
    "\n",
    "net = RecurrentNN(lay_num=1)\n",
    "\n",
    "import random\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.0001, momentum=0.9)\n",
    "writer = SummaryWriter()\n",
    "for epoch in range(50):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    random.seed(42)\n",
    "    random.shuffle(x_batches)\n",
    "    total_norm = 0\n",
    "    for i, data in enumerate(x_batches, 0):\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        zeros = torch.zeros(64,1)\n",
    "        target = data.clone()\n",
    "        target = target.T[1:]\n",
    "        target = target.T\n",
    "        target = torch.cat((target, zeros.long()), 1)\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(data)\n",
    "        outputs = outputs.reshape(outputs.shape[0],outputs.shape[2],outputs.shape[1])\n",
    "        loss = criterion(outputs, target)/len(i2w)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), max_norm=2.0, norm_type=2)\n",
    "        optimizer.step()\n",
    "        # print statistics\n",
    "        for p in net.parameters():\n",
    "            param_norm = p.grad.data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "        total_norm = total_norm ** (1. / 2)\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "    writer.add_scalar('gd/norm', total_norm/len(x_batches), epoch)\n",
    "    writer.add_scalar('losstrain', running_loss/len(x_batches), epoch)\n",
    "    print(f'Finished Epoch {epoch+1}')\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "27d3c36069f2f20e79cdbd3f5c9c5adbcd2046db24a85a953f2d2b5c0bda54b2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}